{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "One-state-problem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uchibe/kyutech-lectures/blob/develop/One_state_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dce_hpK8z0xp"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISpyWJJ6z0xs"
      },
      "source": [
        "# Action selection\n",
        "## Epsilon-greedy policy\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\pi (a) =\n",
        "  \\begin{cases}\n",
        "    \\epsilon/\\lvert A \\rvert + 1 - \\epsilon & \\mathrm{if} \\; \\;\n",
        "       a = \\mathrm{arg} \\max_a Q(a) \\\\\n",
        "    \\epsilon/\\lvert A \\rvert & \\mathrm{otherwise}\n",
        "  \\end{cases}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOF9P_5Yz0xt"
      },
      "source": [
        "def epsilon_greedy_policy(Q, epsilon):\n",
        "    n_actions = Q.shape[0]\n",
        "    policy = epsilon/n_actions*np.ones(n_actions)\n",
        "    policy[Q.argmax(0)] += 1 - epsilon\n",
        "\n",
        "    return policy\n",
        "\n",
        "def plot_Q_function():\n",
        "    action = np.arange(Q_example.shape[0])\n",
        "    %%opts Bars [height=250 width=300]\n",
        "    return hv.Bars((action, Q_example), kdims=['action a'], vdims=['action value Q(a)'])\n",
        "\n",
        "\n",
        "def plot_epsilon_greedy_policy(Q, epsilon=0.1):\n",
        "    policy = epsilon_greedy_policy(Q, epsilon)\n",
        "    action = np.arange(Q.shape[0])\n",
        "\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    axarr = fig.subplots(1, 2)\n",
        "    axarr[0].bar(action, Q)\n",
        "    axarr[0].set_ylabel('action value Q(a)')\n",
        "\n",
        "    axarr[1].bar(action, policy, label=r'$\\epsilon$ = %3.1f' % epsilon)\n",
        "    axarr[1].set_xlabel('action a')\n",
        "    axarr[1].set_ylabel(r'policy $\\pi$ (a)')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp7GInrYz0xv"
      },
      "source": [
        "Q = np.array([2.5, -2.5, 2.9, 1.2, 0.5])\n",
        "plot_epsilon_greedy_policy(Q, epsilon=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT4ymk3Jz0xy"
      },
      "source": [
        "## Boltzmann distribution\n",
        "\\begin{equation*}\n",
        "  \\pi (a) = \\frac{ \\exp (\\beta Q(a)) }{\\sum_{a'} \\exp (\\beta Q(a')) }\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y85hUOgzz0xz"
      },
      "source": [
        "def Boltzmann_policy(Q, beta=1.0):\n",
        "    Qmax = np.max(Q)\n",
        "    expQ = np.exp(beta * (Q - Qmax))\n",
        "    policy = expQ / np.sum(expQ)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "def plot_Boltzmann_policy(Q, beta):\n",
        "    policy = Boltzmann_policy(Q, beta)\n",
        "    action = np.arange(Q.shape[0])\n",
        "\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    axarr = fig.subplots(1, 2)\n",
        "    axarr[0].bar(action, Q)\n",
        "    axarr[0].set_xlabel('action a')\n",
        "    axarr[0].set_ylabel('action value Q(a)')\n",
        "\n",
        "    axarr[1].bar(action, policy, label=r'$\\beta$ = %3.1f' % beta)\n",
        "    axarr[1].set_xlabel('action a')\n",
        "    axarr[1].set_ylabel(r'policy $\\pi$ (a)')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYEi3vugz0x1"
      },
      "source": [
        "Q = 0.1*np.array([2.5, -2.5, 2.9, 1.2, 0.5])\n",
        "plot_Boltzmann_policy(Q, beta=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAU0jU1vz0x3"
      },
      "source": [
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "\n",
        "class GaussianBandit(gym.core.Env):\n",
        "    def __init__(self):\n",
        "        self.n_machines = 5\n",
        "        self.action_space = gym.spaces.Discrete(self.n_machines)\n",
        "        self.observation_space = gym.spaces.Discrete(1)\n",
        "\n",
        "        self.mu = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "        self.sigma = np.array([2.0, 3.0, 2.0, 3.0, 2.0])\n",
        "\n",
        "    def _step(self, action):\n",
        "        done = True\n",
        "        reward = np.random.normal(self.mu[action], self.sigma[action])\n",
        "        return np.array([0]), reward, done, {}\n",
        "\n",
        "    def _reset(self):\n",
        "        return np.array([0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWZKTllGz0x5"
      },
      "source": [
        "## Value-based method\n",
        "\n",
        "\\begin{equation*}\n",
        "  Q(a) = Q(a) + \\alpha (r - Q(a)),\n",
        "\\end{equation*}\n",
        "where $\\alpha$ is a learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhRBcPpsz0x5"
      },
      "source": [
        "import sys\n",
        "def value_based_method(method, beta=1.0, number_of_steps=1000):\n",
        "    env = GaussianBandit()\n",
        "\n",
        "    Q = np.zeros([env.action_space.n, number_of_steps+1])   # initialize an action value function\n",
        "    N = np.zeros([env.action_space.n, number_of_steps+1])   # number of times \"a\" taken\n",
        "    average_reward = 0\n",
        "\n",
        "    for t in range(number_of_steps):\n",
        "        if method == 'random':\n",
        "            policy = Boltzmann_policy(Q[:, t], beta=0.0)\n",
        "            action = np.random.choice(range(env.action_space.n), p=policy)\n",
        "        elif method == 'Boltzmann': \n",
        "            policy = Boltzmann_policy(Q[:, t], beta=beta)\n",
        "            action = np.random.choice(range(env.action_space.n), p=policy)\n",
        "        else:\n",
        "          sys.exit()  \n",
        "        _, reward, _, _ = env._step(action)\n",
        "        \n",
        "        # compute the learning rate\n",
        "        N[:, t+1] = N[:, t] + np.identity(env.action_space.n)[action]\n",
        "        Q[:, t+1] = Q[:, t] # unselected actions\n",
        "        alpha = 1/(N[action, t+1]) # learning rate\n",
        "        # update the action value function\n",
        "        Q[action,t+1] = Q[action,t] + alpha*(reward - Q[action,t])\n",
        "        \n",
        "        average_reward += reward\n",
        "    \n",
        "    # plot the learning curves of the action values\n",
        "    average_reward /= number_of_steps\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    axarr = fig.subplots(1, 2)\n",
        "    for i in range(env.action_space.n):\n",
        "        axarr[0].plot(Q[i, :], label='$a%i$' % i)\n",
        "    axarr[0].legend()\n",
        "    axarr[0].set_xlabel('steps')\n",
        "    axarr[1].set_ylabel('action value')\n",
        "    axarr[0].grid()\n",
        "    for i in range(env.action_space.n):\n",
        "        axarr[1].plot(N[i, :], label='$N%i$' % i)\n",
        "    axarr[1].legend()\n",
        "    axarr[1].set_xlabel('steps')\n",
        "    axarr[1].set_ylabel('N')\n",
        "    axarr[1].grid\n",
        "    \n",
        "    print(Q[:, -1])\n",
        "    print('average reward: %f' % average_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ouYpMMz0x7"
      },
      "source": [
        "value_based_method('random')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wry9EOsyz0x-"
      },
      "source": [
        "value_based_method('Boltzmann', beta=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLncOHaYz0yA"
      },
      "source": [
        "## Gradient descent\n",
        "\n",
        "Consider the minimization problem: $\\min_\\theta J(x)$, where $J(x)$ is differentiable. The local optimal solution is obtained by applying gradient descent:\n",
        "\\begin{equation*}\n",
        "x = x - \\alpha \\nabla_{x} J(x)\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZrz8mg8z0yA"
      },
      "source": [
        "def eval_objective(x):\n",
        "    Jx = 10*np.power(x, 2)\n",
        "    dJx = 20*x\n",
        "    return Jx, dJx\n",
        "\n",
        "def optimize_by_gd(x_init, n_iterations, alpha):\n",
        "    x = np.zeros(n_iterations+1)\n",
        "    x[0] = x_init\n",
        "    y = np.zeros(n_iterations+1)\n",
        "    dy = np.zeros(n_iterations+1)\n",
        "    for i in range(n_iterations):\n",
        "        y[i], dy[i] = eval_objective(x[i])\n",
        "        x[i+1] = x[i] - alpha*dy[i]\n",
        "    y[-1], dy[-1] = eval_objective(x[-1])\n",
        "    return x, y, dy\n",
        "\n",
        "\n",
        "def plot_result(x, y, dy):\n",
        "    xx = np.arange(-100, 50)\n",
        "    Jx, _ = eval_objective(xx)\n",
        "    \n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    axarr = fig.subplots(1, 2)\n",
        "    axarr[0].plot(xx, Jx)\n",
        "    axarr[0].plot(x, y, 'ro-')\n",
        "    axarr[0].set_xlabel('x')\n",
        "    axarr[0].set_ylabel('J(x)')\n",
        "    axarr[1].plot(dy, 'ro-')\n",
        "    axarr[1].set_xlabel('iteration')\n",
        "    axarr[1].set_ylabel('dJ(x)/dx')\n",
        "\n",
        "x, y, dy = optimize_by_gd(x_init=-75, n_iterations=5, alpha=0.1)\n",
        "plot_result(x, y, dy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kWtFuooz0yC"
      },
      "source": [
        "## Policy-based method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AknJQA7Mz0yD"
      },
      "source": [
        "def policy_based_method(method='reduction', number_of_steps=1000):\n",
        "    env = GaussianBandit()\n",
        "\n",
        "    theta = np.zeros([env.action_space.n, number_of_steps+1])   # initialize a policy parameter\n",
        "    N = np.zeros([env.action_space.n, number_of_steps+1])   # number of times \"a\" taken\n",
        "    average_reward = 0\n",
        "\n",
        "    for t in range(number_of_steps):\n",
        "        # compute a policy from the policy parameters\n",
        "        policy = Boltzmann_policy(theta[:, t], beta=1.0)\n",
        "        action = np.random.choice(range(env.action_space.n), p=policy)\n",
        "        _, reward, _, _ = env._step(action)\n",
        "        \n",
        "        alpha = 0.05 # learning rate\n",
        "        gradient = np.identity(env.action_space.n)[action] - policy\n",
        "        if method == 'reduction':\n",
        "            theta[:, t+1] = theta[:, t] + alpha*(reward - average_reward/(t+1))*gradient \n",
        "        elif method == 'no reduction':\n",
        "            theta[:, t+1] = theta[:, t] + alpha*reward*gradient\n",
        "        else:\n",
        "            sys.exit()\n",
        "        \n",
        "        average_reward += reward\n",
        "    \n",
        "    # plot the learning curves of the action values\n",
        "    average_reward /= number_of_steps\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    axarr = fig.subplots(1, 2)\n",
        "    for i in range(env.action_space.n):\n",
        "        axarr[0].plot(theta[i, :], label='$a%i$' % i)\n",
        "    axarr[0].legend()\n",
        "    axarr[0].set_xlabel('steps')\n",
        "    axarr[0].set_ylabel('policy parameter theta')\n",
        "    axarr[0].grid()\n",
        "#   for i in range(env.action_space.n):\n",
        "#        axarr[1].plot(N[i, :], label='$N%i$' % i)\n",
        "#    axarr[1].legend()\n",
        "#    axarr[1].set_xlabel('steps')\n",
        "#    axarr[1].set_ylabel('N')\n",
        "#    axarr[1].grid\n",
        "\n",
        "    print('average reward: %f' % average_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO_otS2pz0yF"
      },
      "source": [
        "policy_based_method('no reduction')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}