{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook of Reinforcement Learning for Bandit Problems\n",
    "This notebook was developed to explain a bandit problem, which is formally equivalent to a one-state Markov Decision Process (MDP). It is defined by a tuple $(\\mathcal{A}, p)$, where $\\mathcal{A}$ is a finite set of actions, and $p$ represents the probability for the reward given the current action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dce_hpK8z0xp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from gym import Env, spaces, utils\n",
    "from ipywidgets import interact, fixed\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the bandit problem\n",
    "There are five slot machines, and therefore, an agent has five discrete actions. The agent receives a scalar reward according to the choice of actions. The goal is to find an action that maximizes the expected reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAU0jU1vz0x3"
   },
   "outputs": [],
   "source": [
    "class GaussianBandit(Env):\n",
    "    def __init__(self):\n",
    "        self.n_machines = 5\n",
    "        self.action_space = spaces.Discrete(self.n_machines)\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "\n",
    "        self.mu = np.array([1.0, 5.0, 3.0, 2.0, 3.5])\n",
    "        self.sigma = np.array([0.8, 0.9, 1.5, 1.5, 1.2])\n",
    "\n",
    "    def _step(self, action):\n",
    "        done = True\n",
    "        reward = np.random.normal(self.mu[action], self.sigma[action])\n",
    "        return np.array([0]), reward, done, {}\n",
    "\n",
    "    def _reset(self):\n",
    "        return np.array([0])\n",
    "\n",
    "    def plot(self):\n",
    "        fig = plt.figure(figsize=(8,5))\n",
    "        axarr = fig.subplots(1, 1)\n",
    "\n",
    "        x = np.arange(start=-3.0, stop=8.0, step=0.05)\n",
    "        for index, mean in enumerate(self.mu):\n",
    "            std = self.sigma[index]\n",
    "            norm_pdf = stats.norm.pdf(x=x, loc=mean, scale=std)\n",
    "            axarr.plot(x, norm_pdf, label='r%1.0f' % index)\n",
    "        axarr.legend()\n",
    "        axarr.set_xlabel('r')\n",
    "        axarr.set_ylabel('pdf')\n",
    "        axarr.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the reward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GaussianBandit()\n",
    "env.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISpyWJJ6z0xs"
   },
   "source": [
    "# Action selection\n",
    "## $\\varepsilon$-greedy policy\n",
    "$\\varepsilon$-greedy is a method to balance exploration and exploitation in the value-based reinforcement learning. When the currest estimate of the action-value function is $Q(a)$, the probability is to select the action $a$ is given by\n",
    "\\begin{equation}\n",
    "  \\pi (a) =\n",
    "  \\begin{cases}\n",
    "    \\epsilon/\\lvert A \\rvert + 1 - \\epsilon & \\mathrm{if} \\; \\;\n",
    "       a = \\mathrm{arg} \\max_a Q(a), \\\\\n",
    "    \\epsilon/\\lvert A \\rvert & \\mathrm{otherwise},\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "where $\\varepsilon \\in [0, 1]$ is a hyperparameter and $\\lvert A \\rvert$ is the number of available actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOF9P_5Yz0xt"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, epsilon=0.1):\n",
    "    \"\"\"Epsilon-greedy policy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : ndarray\n",
    "        action-value function (1D array).\n",
    "    epsilon : float\n",
    "        hyperparameter to control the tradeoff between exploration and\n",
    "        exploitation.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    policy : ndarray\n",
    "        probability to select an action (1D array).\n",
    "    \"\"\"\n",
    "    n_actions = Q.shape[0]\n",
    "    policy = epsilon/n_actions*np.ones(n_actions)\n",
    "    policy[Q.argmax(0)] += 1 - epsilon\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def plot_epsilon_greedy_policy(Q, epsilon=0.1):\n",
    "    \"\"\"Plot the epsilon-greedy policy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : ndarray\n",
    "        action-value function (1D array).\n",
    "    epsilon : float\n",
    "        hyperparameter to control the tradeoff between exploration and\n",
    "        exploitation.\n",
    "\n",
    "    \"\"\"\n",
    "    policy = epsilon_greedy_policy(Q, epsilon)\n",
    "    action = np.arange(Q.shape[0])\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    axarr = fig.subplots(1, 2)\n",
    "    axarr[0].bar(action, Q)\n",
    "    axarr[0].set_xlabel('action a')\n",
    "    axarr[0].set_ylabel('action value Q(a)')\n",
    "\n",
    "    axarr[1].bar(action, policy, label=r'$\\epsilon$ = %3.1f' % epsilon)\n",
    "    axarr[1].set_xlabel('action a')\n",
    "    axarr[1].set_ylabel(r'policy $\\pi$ (a)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pp7GInrYz0xv"
   },
   "outputs": [],
   "source": [
    "# epsilon = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "Q = np.array([2.5, -2.5, 2.9, 1.2, 0.5])\n",
    "interact(plot_epsilon_greedy_policy, Q=fixed(Q), epsilon=(0, 1, 0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT4ymk3Jz0xy"
   },
   "source": [
    "## Boltzmann distribution\n",
    "The Boltzmann distribution is an alternative method for action selection. The probability distribution is defined by\n",
    "\\begin{equation}\n",
    "  \\pi (a) = \\frac{ \\exp (\\beta Q(a)) }{\\sum_{a'} \\exp (\\beta Q(a')) },\n",
    "\\end{equation}\n",
    "where $\\beta$ is a non-negative hyperparameter, which is often called the inverse temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y85hUOgzz0xz"
   },
   "outputs": [],
   "source": [
    "def Boltzmann_policy(Q, beta=1.0):\n",
    "    \"\"\"Boltzmann policy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : ndarray\n",
    "        action-value function (1D array).\n",
    "    beta : float\n",
    "        hyperparameter to control the tradeoff between exploration and\n",
    "        exploitation.\n",
    "\n",
    "    \"\"\"\n",
    "    Qmax = np.max(Q)\n",
    "    expQ = np.exp(beta * (Q - Qmax))\n",
    "    policy = expQ / np.sum(expQ)\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def plot_Boltzmann_policy(Q, beta=1.0):\n",
    "    \"\"\"Plot the Boltzmann policy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : ndarray\n",
    "        action-value function (1D array).\n",
    "    beta : float\n",
    "        hyperparameter to control the tradeoff between exploration and\n",
    "        exploitation.\n",
    "\n",
    "    \"\"\"\n",
    "    policy = Boltzmann_policy(Q, beta)\n",
    "    action = np.arange(Q.shape[0])\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    axarr = fig.subplots(1, 2)\n",
    "    axarr[0].bar(action, Q)\n",
    "    axarr[0].set_xlabel('action a')\n",
    "    axarr[0].set_ylabel('action value Q(a)')\n",
    "\n",
    "    axarr[1].bar(action, policy, label=r'$\\beta$ = %3.1f' % beta)\n",
    "    axarr[1].set_xlabel('action a')\n",
    "    axarr[1].set_ylabel(r'policy $\\pi$ (a)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYEi3vugz0x1"
   },
   "outputs": [],
   "source": [
    "# beta = 1 #@param {type:\"slider\", min:0, max:5, step:0.2}\n",
    "Q = np.array([2.5, -2.5, 2.9, 1.2, 0.5])\n",
    "interact(plot_Boltzmann_policy, Q=fixed(Q), beta=(0, 8, 0.2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWZKTllGz0x5"
   },
   "source": [
    "# Value-based method\n",
    "\n",
    "An agent learns an expected reward. When the agent selects an action $a$ and receives a scalar reward $r$, an estimate of the expected reward is updated by\n",
    "\\begin{equation*}\n",
    "  Q(a) = Q(a) + \\alpha (r - Q(a)),\n",
    "\\end{equation*}\n",
    "where $\\alpha$ is a learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhRBcPpsz0x5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def value_based_method(method, beta=1.0, epsilon=0.1, number_of_steps=1000):\n",
    "    env = GaussianBandit()\n",
    "\n",
    "    Q = np.zeros([env.action_space.n, number_of_steps+1])   # initialize an action value function\n",
    "    N = np.zeros([env.action_space.n, number_of_steps+1])   # number of times \"a\" taken\n",
    "    average_reward = 0\n",
    "\n",
    "    for t in range(number_of_steps):\n",
    "        if method == 'epsilon':\n",
    "            policy = epsilon_greedy_policy(Q[:, t], epsilon=epsilon)\n",
    "            action = np.random.choice(range(env.action_space.n), p=policy)\n",
    "        elif method == 'Boltzmann': \n",
    "            policy = Boltzmann_policy(Q[:, t], beta=beta)\n",
    "            action = np.random.choice(range(env.action_space.n), p=policy)\n",
    "        else:\n",
    "          sys.exit()  \n",
    "        _, reward, _, _ = env._step(action)\n",
    "        \n",
    "        # compute the learning rate\n",
    "        N[:, t+1] = N[:, t] + np.identity(env.action_space.n)[action]\n",
    "        Q[:, t+1] = Q[:, t] # unselected actions\n",
    "        alpha = 1/(N[action, t+1]) # learning rate\n",
    "        # update the action value function\n",
    "        Q[action,t+1] = Q[action,t] + alpha*(reward - Q[action,t])\n",
    "        \n",
    "        average_reward += reward\n",
    "    \n",
    "    # plot the learning curves of the action values\n",
    "    average_reward /= number_of_steps\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    axarr = fig.subplots(1, 2)\n",
    "    for i in range(env.action_space.n):\n",
    "        axarr[0].plot(Q[i, :], label='$a%i$' % i)\n",
    "    axarr[0].legend()\n",
    "    axarr[0].set_xlabel('steps')\n",
    "    axarr[0].set_ylabel('action value Q(a)')\n",
    "    axarr[0].grid()\n",
    "    for i in range(env.action_space.n):\n",
    "        axarr[1].plot(N[i, :], label='$N%i$' % i)\n",
    "    axarr[1].legend()\n",
    "    axarr[1].set_xlabel('steps')\n",
    "    axarr[1].set_ylabel('N (a)')\n",
    "    axarr[1].grid\n",
    "    \n",
    "    print(Q[:, -1])\n",
    "    print('average reward: %f' % average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29ouYpMMz0x7"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "value_based_method('epsilon', epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wry9EOsyz0x-"
   },
   "outputs": [],
   "source": [
    "beta = 1 #@param {type:\"slider\", min:0, max:5, step:0.2}\n",
    "value_based_method('Boltzmann', beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLncOHaYz0yA"
   },
   "source": [
    "# Policy-based method\n",
    "The policy-based method directly learns a stochastic policy. The basic idea is to utilize the gradient descent (ascent) method to update the parameters of the stochastic policy. \n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Consider the minimization problem: $\\min_x J(x)$, where $J(x)$ is differentiable. The local optimal solution is obtained by applying gradient descent:\n",
    "\\begin{equation}\n",
    "x = x - \\alpha \\nabla_{x} J(x),\n",
    "\\end{equation}\n",
    "where $\\nabla_{x} J(x)$ is a partial derivative of $J(x)$ with respect to $x$, and $\\alpha$ is a non-negative stepsize hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZrz8mg8z0yA"
   },
   "outputs": [],
   "source": [
    "def eval_objective(x):\n",
    "    Jx = 10*np.power(x, 2)\n",
    "    dJx = 20*x\n",
    "    return Jx, dJx\n",
    "\n",
    "def optimize_by_gd(x_init, n_iterations, alpha):\n",
    "    x = np.zeros(n_iterations+1)\n",
    "    x[0] = x_init\n",
    "    y = np.zeros(n_iterations+1)\n",
    "    dy = np.zeros(n_iterations+1)\n",
    "    for i in range(n_iterations):\n",
    "        y[i], dy[i] = eval_objective(x[i])\n",
    "        x[i+1] = x[i] - alpha*dy[i]\n",
    "    y[-1], dy[-1] = eval_objective(x[-1])\n",
    "    return x, y, dy\n",
    "\n",
    "\n",
    "def plot_result(x, y, dy):\n",
    "    xx = np.arange(-100, 100)\n",
    "    Jx, _ = eval_objective(xx)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    axarr = fig.subplots(1, 2)\n",
    "    axarr[0].plot(xx, Jx)\n",
    "    axarr[0].plot(x, y, 'ro-')\n",
    "    axarr[0].set_xlabel('x')\n",
    "    axarr[0].set_ylabel('J(x)')\n",
    "    axarr[1].plot(dy, 'ro-')\n",
    "    axarr[1].set_xlabel('iteration')\n",
    "    axarr[1].set_ylabel('dJ(x)/dx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of gradient descent\n",
    "The following is the simple example of the gradient descent algorithm. The learning rate $\\alpha$ affects convergence, and you can see that the algorithm never converges if $\\alpha = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPQzfbpRoh9x"
   },
   "outputs": [],
   "source": [
    "alpha = 0.01 #@param {type:\"slider\", min:0.01, max:0.1, step:0.005}\n",
    "x, y, dy = optimize_by_gd(x_init=-75, n_iterations=50, alpha=alpha)\n",
    "plot_result(x, y, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kWtFuooz0yC"
   },
   "source": [
    "## Policy-based method\n",
    "The Boltzmann distribution is chosen because the $\\varepsilon$-greedy is not differentiable due to the max operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AknJQA7Mz0yD"
   },
   "outputs": [],
   "source": [
    "def policy_based_method(method='reduction', alpha=0.05, number_of_steps=1000):\n",
    "    env = GaussianBandit()\n",
    "\n",
    "    theta = np.zeros([env.action_space.n, number_of_steps+1])   # initialize a policy parameter\n",
    "    N = np.zeros([env.action_space.n, number_of_steps+1])   # number of times \"a\" taken\n",
    "    average_reward = 0\n",
    "\n",
    "    for t in range(number_of_steps):\n",
    "        # compute a policy from the policy parameters\n",
    "        policy = Boltzmann_policy(theta[:, t], beta=1.0)\n",
    "        action = np.random.choice(range(env.action_space.n), p=policy)\n",
    "        _, reward, _, _ = env._step(action)\n",
    "        \n",
    "        N[:, t+1] = N[:, t] + np.identity(env.action_space.n)[action]\n",
    "        gradient = np.identity(env.action_space.n)[action] - policy\n",
    "        if method == 'reduction':\n",
    "            theta[:, t+1] = theta[:, t] + alpha*(reward - average_reward/(t+1))*gradient \n",
    "        elif method == 'no reduction':\n",
    "            theta[:, t+1] = theta[:, t] + alpha*reward*gradient\n",
    "        else:\n",
    "            sys.exit()\n",
    "        \n",
    "        average_reward += reward\n",
    "    \n",
    "    # plot the learning curves of the action values\n",
    "    average_reward /= number_of_steps\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    axarr = fig.subplots(1, 2)\n",
    "    for i in range(env.action_space.n):\n",
    "        axarr[0].plot(theta[i, :], label='$a%i$' % i)\n",
    "    axarr[0].legend()\n",
    "    axarr[0].set_xlabel('steps')\n",
    "    axarr[0].set_ylabel('policy parameter theta')\n",
    "    axarr[0].grid()\n",
    "    for i in range(env.action_space.n):\n",
    "        axarr[1].plot(N[i, :], label='$N%i$' % i)\n",
    "    axarr[1].legend()\n",
    "    axarr[1].set_xlabel('steps')\n",
    "    axarr[1].set_ylabel('N')\n",
    "    axarr[1].grid\n",
    "\n",
    "    print('average reward: %f' % average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qO_otS2pz0yF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.03 #@param {type:\"slider\", min:0.01, max:0.1, step:0.005} \n",
    "policy_based_method('no reduction', alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbhlqDVeoGOb"
   },
   "outputs": [],
   "source": [
    "alpha = 0.03 #@param {type:\"slider\", min:0.01, max:0.1, step:0.005} \n",
    "policy_based_method('reduction', alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwZySomL5UpL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "One-state-problem.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
