{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twl2rlmg1tW-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import convolve\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlGEc3RC1tXB"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Epsilon greedy policy\n",
    "    \"\"\"\n",
    "    n_actions = Q.shape[0]\n",
    "    policy = epsilon/n_actions*np.ones(n_actions)\n",
    "    policy[Q.argmax(0)] += 1 - epsilon\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def sample_epsilon_greedy(Q, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Sample an action from an epsilon greedy policy\n",
    "    \"\"\"\n",
    "    n_actions = Q.shape[0]\n",
    "    prob = epsilon_greedy_policy(Q, epsilon)\n",
    "    return np.random.choice(n_actions, p=prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geiym8eb1tXD"
   },
   "source": [
    "## Value-based update rules\n",
    "When $s_t$ is a terminal state, the TD error is computed by\n",
    "\\begin{equation*}\n",
    "  \\delta = r_t - Q(s_p, a_p)\n",
    "\\end{equation*}\n",
    "Otherwise, the TD error is computed by\n",
    "\\begin{equation*}\n",
    "  \\delta = \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chPBHf-Y1tXD"
   },
   "outputs": [],
   "source": [
    "GAMMA = 1.0\n",
    "def calc_TD(method, Q, s_p, a_p, r_t, s_t, a_t, is_terminal):\n",
    "    if is_terminal:\n",
    "        td_error = r_t - Q[s_p, a_p]\n",
    "    elif method == 'qlearn':\n",
    "        td_error = r_t + GAMMA*np.max(Q[s_t, :]) - Q[s_p, a_p]\n",
    "    elif method == 'sarsa':\n",
    "        td_error = r_t + GAMMA*Q[s_t, a_t] - Q[s_p, a_p]\n",
    "    elif method == 'expsarsa':\n",
    "        prob = epsilon_greedy_policy(Q[s_t, :], epsilon=EPSILON)\n",
    "        Q_s_t = np.dot(prob, Q[s_t, :])\n",
    "        td_error = r_t + GAMMA*Q_s_t - Q[s_p, a_p]\n",
    "    else:\n",
    "        td_error = r_t + GAMMA*np.max(Q[s_t, :]) - Q[s_p, a_p]\n",
    "\n",
    "    return td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_eRQP_r1tXF"
   },
   "outputs": [],
   "source": [
    "NUM_EPISODES = 1000\n",
    "ALPHA = 0.2\n",
    "EPSILON = 0.1\n",
    "\n",
    "def run(method):\n",
    "    env = gym.make('CliffWalking-v0', new_step_api=False)\n",
    "\n",
    "    # state-action value function\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    total_reward = np.zeros(NUM_EPISODES)\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        s_p, is_terminal = env.reset(), False\n",
    "        a_p = sample_epsilon_greedy(Q[s_p, :], epsilon=EPSILON)\n",
    "\n",
    "        while not is_terminal:\n",
    "            s_t, r_t, is_terminal, _ = env.step(a_p)\n",
    "            a_t = sample_epsilon_greedy(Q[s_t, :], epsilon=EPSILON)\n",
    "\n",
    "            td_error = calc_TD(method, Q, s_p, a_p, r_t, s_t, a_t, is_terminal)\n",
    "            Q[s_p, a_p] += ALPHA*td_error\n",
    "\n",
    "            s_p, a_p = s_t, a_t\n",
    "\n",
    "            total_reward[episode] += r_t\n",
    "            # if episode == NUM_EPISODES-1:\n",
    "            #     env.render()\n",
    "\n",
    "    return Q, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOnJjuv11tXH"
   },
   "outputs": [],
   "source": [
    "def plot_rewards(r_qlearn, r_sarsa):\n",
    "    plt.plot(np.mean(r_qlearn, axis=0), label='Q learning')\n",
    "    plt.plot(np.mean(r_sarsa, axis=0), label='SARSA')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('sum of rewards')\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([-100, 0])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d0SWPqI1tXJ"
   },
   "outputs": [],
   "source": [
    "NUM_RUNS = 10\n",
    "\n",
    "r_qlearn = np.zeros((NUM_RUNS, NUM_EPISODES))\n",
    "r_sarsa = np.zeros((NUM_RUNS, NUM_EPISODES))\n",
    "\n",
    "for r in range(NUM_RUNS):\n",
    "    q_qlearn, r_qlearn[r, :] = run('qlearn')\n",
    "    q_sarsa, r_sarsa[r, :] = run('sarsa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "vOIk9tj35FQF",
    "outputId": "c69233e5-8d84-47f5-b621-754dbc87ce2c"
   },
   "outputs": [],
   "source": [
    "plot_rewards(r_qlearn, r_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTz3TM3L1tXM"
   },
   "outputs": [],
   "source": [
    "def evaluate(Q):\n",
    "    env = gym.make('CliffWalking-v0', new_step_api=False)\n",
    "\n",
    "    s_p, is_terminal = env.reset(), False\n",
    "    a_p = sample_epsilon_greedy(Q[s_p, :], epsilon=0) # select the optimal action\n",
    "\n",
    "    while not is_terminal:\n",
    "        print(env.render(mode='ansi'))\n",
    "        s_t, r_t, is_terminal, _ = env.step(a_p)\n",
    "        a_t = sample_epsilon_greedy(Q[s_t, :], epsilon=0)\n",
    "        s_p, a_p = s_t, a_t\n",
    "    print(env.render(mode='ansi'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f_DDx7bh1tXO",
    "outputId": "d4e9f753-f6ef-4503-ae02-d5c0fcebfe51"
   },
   "outputs": [],
   "source": [
    "evaluate(q_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(q_qlearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Cliff_environment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
