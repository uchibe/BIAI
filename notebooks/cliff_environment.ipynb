{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook of Model-Free Reinforcement Learning\n",
    "This notebook implements two model-free reinforcement learning algorithms, SARSA and Q-learning. They are evaluated on the [Cliff-Walking task](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/) provided by the [OpenAI gym](https://github.com/openai/gym). There are 3x12 + 1 possible states and 4 discrete deterministic actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twl2rlmg1tW-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('CliffWalking-v0', new_step_api=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the $\\varepsilon$-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlGEc3RC1tXB"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, epsilon=0.1):\n",
    "    \"\"\"Epsilon greedy policy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : ndarray\n",
    "        action-value function (1D array).\n",
    "    epsilon : float\n",
    "        hyperparameter to control the tradeoff between exploration and\n",
    "        exploitation.\n",
    "    \"\"\"\n",
    "    n_actions = Q.shape[0]\n",
    "    policy = epsilon/n_actions*np.ones(n_actions)\n",
    "    policy[Q.argmax(0)] += 1 - epsilon\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def sample_epsilon_greedy(Q, epsilon=0.1):\n",
    "    \"\"\"Sample an action by the epsilon greedy policy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : ndarray\n",
    "        action-value function (1D array).\n",
    "    epsilon : float\n",
    "        hyperparameter to control the tradeoff between exploration and\n",
    "        exploitation.    \"\"\"\n",
    "    n_actions = Q.shape[0]\n",
    "    prob = epsilon_greedy_policy(Q, epsilon)\n",
    "\n",
    "    return np.random.choice(n_actions, p=prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geiym8eb1tXD"
   },
   "source": [
    "# Update rules of SARSA and Q-learning\n",
    "When $s_t$ is a terminal state, the TD error is computed by\n",
    "\\begin{equation}\n",
    "  \\delta = r_t - Q(s_p, a_p).\n",
    "\\end{equation}\n",
    "Otherwise, the TD error of SARSA is computed by\n",
    "\\begin{equation}\n",
    "  \\delta = r_t + \\gamma Q(s_t, a_t) - Q(s_p, a_p),\n",
    "\\end{equation}\n",
    "while the TD-error of Q-learning is given by\n",
    "\\begin{equation}\n",
    "  \\delta = r_t + \\gamma \\max_a Q(s_t, a) - Q(s_p, a_p).\n",
    "\\end{equation}\n",
    "Then, $Q(s_p, a_p)$ is updated by\n",
    "\\begin{equation}\n",
    "  Q(s_p, a_p) = Q(s_p, a_p) + \\alpha \\delta,\n",
    "\\end{equation}\n",
    "where $\\alpha \\in [0, 1]$ is a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chPBHf-Y1tXD"
   },
   "outputs": [],
   "source": [
    "def calc_TD(method, Q, s_p, a_p, r_t, s_t, a_t, is_terminal, gamma=1.0):\n",
    "    \"\"\"Calculate TD error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    method : str\n",
    "    Q : ndarray\n",
    "    s_p : int\n",
    "    a_p : int\n",
    "    r_t : float\n",
    "    s_t : int\n",
    "    a_t : int\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    td_error : float\n",
    "        TD error.\n",
    "    \"\"\"\n",
    "    if is_terminal:\n",
    "        td_error = r_t - Q[s_p, a_p]\n",
    "    elif method == 'qlearn':\n",
    "        td_error = r_t + gamma*np.max(Q[s_t, :]) - Q[s_p, a_p]\n",
    "    elif method == 'sarsa':\n",
    "        td_error = r_t + gamma*Q[s_t, a_t] - Q[s_p, a_p]\n",
    "    elif method == 'expsarsa':\n",
    "        prob = epsilon_greedy_policy(Q[s_t, :], epsilon=EPSILON)\n",
    "        Q_s_t = np.dot(prob, Q[s_t, :])\n",
    "        td_error = r_t + gamma*Q_s_t - Q[s_p, a_p]\n",
    "    else:\n",
    "        td_error = r_t + gamma*np.max(Q[s_t, :]) - Q[s_p, a_p]\n",
    "\n",
    "    return td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of the Cliff-Walking task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_eRQP_r1tXF"
   },
   "outputs": [],
   "source": [
    "def run(method, alpha=0.2, epsilon=0.1, num_episodes=1000):\n",
    "    # state-action value function\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    total_reward = np.zeros(num_episodes)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        s_p, is_terminal = env.reset(), False\n",
    "        a_p = sample_epsilon_greedy(Q[s_p, :], epsilon=epsilon)\n",
    "\n",
    "        while not is_terminal:\n",
    "            s_t, r_t, is_terminal, _ = env.step(a_p)\n",
    "            a_t = sample_epsilon_greedy(Q[s_t, :], epsilon=epsilon)\n",
    "\n",
    "            td_error = calc_TD(method, Q, s_p, a_p, r_t, s_t, a_t, is_terminal)\n",
    "            Q[s_p, a_p] += alpha*td_error\n",
    "\n",
    "            s_p, a_p = s_t, a_t\n",
    "\n",
    "            total_reward[episode] += r_t\n",
    "\n",
    "    return Q, total_reward\n",
    "\n",
    "\n",
    "def plot_rewards(r_sarsa=None, r_qlearn=None):\n",
    "    \"\"\"Plot the learning curves\n",
    "    \"\"\"\n",
    "    def plot_shaded(result, algorithm_name, color):\n",
    "        r_mean = np.mean(result, axis=0)\n",
    "        # r_std = np.std(result, axis=0)\n",
    "        x = np.arange(len(r_mean))\n",
    "        plt.plot(x, r_mean, '-', color=color, label=algorithm_name)\n",
    "        # plt.fill_between(x, r_mean - r_std, r_mean + r_std, color=color, alpha=0.2)\n",
    "        \n",
    "    \n",
    "    if r_sarsa is not None:\n",
    "        plot_shaded(r_sarsa, 'SARSA', 'b')\n",
    "    if r_qlearn is not None:\n",
    "        plot_shaded(r_qlearn, 'Q-learning', 'r')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('sum of rewards')\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([-100, 0])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate(Q):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    s_p, is_terminal = env.reset(), False\n",
    "    a_p = sample_epsilon_greedy(Q[s_p, :], epsilon=0) # select the optimal action\n",
    "\n",
    "    while not is_terminal:\n",
    "        print(env.render(mode='ansi'))\n",
    "        s_t, r_t, is_terminal, _ = env.step(a_p)\n",
    "        a_t = sample_epsilon_greedy(Q[s_t, :], epsilon=0)\n",
    "        s_p, a_p = s_t, a_t\n",
    "    print(env.render(mode='ansi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation results of SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d0SWPqI1tXJ"
   },
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "epsilon = 0.1\n",
    "num_episodes = 1000\n",
    "num_runs = 10\n",
    "r_sarsa = np.zeros((num_runs, num_episodes))\n",
    "for r in range(num_runs):\n",
    "    q_sarsa, r_sarsa[r, :] = run('sarsa', alpha, epsilon, num_episodes)\n",
    "plot_rewards(r_sarsa=r_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation results of Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_qlearn = np.zeros((num_runs, num_episodes))\n",
    "for r in range(num_runs):\n",
    "    q_qlearn, r_qlearn[r, :] = run('qlearn', alpha, epsilon, num_episodes)\n",
    "plot_rewards(r_sarsa=r_sarsa, r_qlearn=r_qlearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTz3TM3L1tXM"
   },
   "source": [
    "## Optimal policy of SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f_DDx7bh1tXO",
    "outputId": "d4e9f753-f6ef-4503-ae02-d5c0fcebfe51"
   },
   "outputs": [],
   "source": [
    "evaluate(q_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal policy of Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(q_qlearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Cliff_environment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
